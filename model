import torch
import torch.nn as nn
import torch.nn.functional as F

class LinearAttention(nn.Module):
    """
    Linear Attention Mechanism from "Simple linear attention language models balance the recall-throughput tradeoff"
    Implements the linear attention with ELU-based feature maps for CPU compatibility
    """
    def __init__(self, dim, head_dim=64):
        super().__init__()
        self.head_dim = head_dim
        self.n_heads = dim // head_dim
        
        self.qkv_proj = nn.Linear(dim, 3*dim)
        self.out_proj = nn.Linear(dim, dim)
        
        # ELU parameters for feature mapping
        self.alpha = 1.0
        self.epsilon = 1e-6

    def elu_feature_map(self, x):
        return F.elu(x) + self.alpha + self.epsilon

    def forward(self, x, mask=None):
        B, T, _ = x.shape
        qkv = self.qkv_proj(x).reshape(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, H, T, D]
        
        # Apply ELU feature maps
        q = self.elu_feature_map(q)
        k = self.elu_feature_map(k)
        
        # Compute attention (matrix product)
        k_v = torch.einsum('bhnd,bhnc->bhdc', k, v)
        Z = 1 / (torch.einsum('bhnd,bhd->bhn', q, k.sum(dim=2)) + 1e-6)
        out = torch.einsum('bhnd,bhdc,bhn->bhnc', q, k_v, Z)
        
        # Combine heads and project
        out = out.reshape(B, T, -1)
        return self.out_proj(out)

class LinearTransformerBlock(nn.Module):
    """Transformer block with linear attention"""
    def __init__(self, dim, mlp_ratio=4):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = LinearAttention(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * mlp_ratio),
            nn.GELU(),
            nn.Linear(dim * mlp_ratio, dim)
        )
        
    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class LinearLM(nn.Module):
    """Linear Attention Language Model"""
    def __init__(self, vocab_size, dim=256, depth=6, n_head=4):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, dim)
        self.blocks = nn.Sequential(*[
            LinearTransformerBlock(dim) for _ in range(depth)
        ])
        self.norm = nn.LayerNorm(dim)
        self.head = nn.Linear(dim, vocab_size)
        
    def forward(self, x):
        x = self.embed(x)
        for block in self.blocks:
            x = block(x)
        return self.head(self.norm(x))
