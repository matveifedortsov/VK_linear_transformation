import torch
import torch.nn as nn
import torch.nn.functional as F

class LinearCausalAttention(nn.Module):
    def __init__(self, dim, heads=8):
        super().__init__()
        self.heads = heads
        self.dim = dim
        self.head_dim = dim // heads
        self.qkv = nn.Linear(dim, 3 * dim)
        self.elu = nn.ELU()

    def feature_map(self, x):
        return self.elu(x) + 1

    def forward(self, x):
        B, T, C = x.shape
        qkv = self.qkv(x)
        q, k, v = qkv.chunk(3, dim=-1)

        q = q.view(B, T, self.heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.heads, self.head_dim).transpose(1, 2)

        q = self.feature_map(q)
        k = self.feature_map(k)

        cumulative = torch.zeros(B, self.heads, self.head_dim, self.head_dim, device=x.device)
        outputs = []

        for i in range(T):
            k_i = k[:, :, i, :]
            v_i = v[:, :, i, :]
            outer = torch.einsum('bhd,bhe->bhde', k_i, v_i)
            cumulative += outer
            q_i = q[:, :, i, :]
            out_i = torch.einsum('bhd,bhde->bhe', q_i, cumulative)
            outputs.append(out_i)

        out = torch.stack(outputs, dim=2)
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return out

class LinearTransformerLM(nn.Module):
    def __init__(self, vocab_size, dim, num_layers, num_heads):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, dim)
        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'attention': LinearCausalAttention(dim, heads=num_heads),
                'ffn': nn.Sequential(
                    nn.Linear(dim, 4 * dim),
                    nn.GELU(),
                    nn.Linear(4 * dim, dim)
                ),
                'norm1': nn.LayerNorm(dim),
                'norm2': nn.LayerNorm(dim)
            })
            for _ in range(num_layers)
        ])
        self.lm_head = nn.Linear(dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        for layer in self.layers:
            attn_out = layer['attention'](layer['norm1'](x))
            x = x + attn_out
            ffn_out = layer['ffn'](layer['norm2'](x))
            x = x + ffn_out
        logits = self.lm_head(x)
        return logits
